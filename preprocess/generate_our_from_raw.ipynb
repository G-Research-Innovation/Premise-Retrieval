{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Generator\n",
    "import re\n",
    "import toml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful link: https://github.com/lean-dojo/LeanDojo/blob/main/scripts/generate-benchmark-lean4.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level pre-processing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser_config = argparse.ArgumentParser()\n",
    "    parser_config.add_argument(\n",
    "        \"--config_path\", required=True, type=str, help=\"Path to the config file\"\n",
    "    )\n",
    "    args_config = parser_config.parse_args()\n",
    "    config = toml.load(args_config.config_path)\n",
    "    all_path_ori = config['dataset']['parent_path']\n",
    "    all_path_save = config['dataset']['parent_save_path_our']\n",
    "    if not os.path.exists(all_path_save):\n",
    "        os.makedirs(all_path_save)\n",
    "\n",
    "    train_path = all_path_ori + \"train.jsonl\"\n",
    "    test_path = all_path_ori + \"test.jsonl\"\n",
    "    val_path = all_path_ori + \"valid.jsonl\"\n",
    "    train_path_save = all_path_save + 'train.jsonl'\n",
    "    test_path_save = all_path_save + 'test.jsonl'\n",
    "    val_path_save = all_path_save + 'val.jsonl'\n",
    "    test_path_save_increase = all_path_save + 'test_increase.jsonl'\n",
    "    val_path_save_increase = all_path_save + 'val_increase.jsonl'\n",
    "    train_expand_path = all_path_save + \"train_expand_premise.jsonl\"\n",
    "\n",
    "    modeul_id = _load_corpus_update(\n",
    "        config['dataset']['all_data_path'] + \"corpus.jsonl\",\n",
    "        config['dataset']['all_data_path'] + 'statement.jsonl',\n",
    "        config['dataset']['all_data_path'] + 'module_id_mapping.json',\n",
    "    )\n",
    "\n",
    "    _get_dataset_path(train_path, train_path_save, modeul_id)\n",
    "    _get_dataset_path(test_path, test_path_save, modeul_id)\n",
    "    _get_dataset_path(val_path, val_path_save, modeul_id)\n",
    "\n",
    "    expand_data(train_path_save, train_expand_path)\n",
    "\n",
    "    _get_dataset_path_test(test_path, test_path_save_increase, is_train=False, modeul_id=modeul_id)\n",
    "    _get_dataset_path_test(val_path, val_path_save_increase, is_train=False, modeul_id=modeul_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(eq=True, unsafe_hash=True)\n",
    "class Pos:\n",
    "    \"\"\"Position in source files.\n",
    "\n",
    "    We use 1-index to keep it consistent with code editors such as Visual Studio Code.\n",
    "    \"\"\"\n",
    "\n",
    "    line_nb: int\n",
    "    \"\"\"Line number\n",
    "    \"\"\"\n",
    "\n",
    "    column_nb: int\n",
    "    \"\"\"Column number\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls, s: str) -> \"Pos\":\n",
    "        \"\"\"Construct a :class:`Pos` object from its string representation, e.g., :code:`\"(323, 1109)\"`.\"\"\"\n",
    "        assert s.startswith(\"(\") and s.endswith(\n",
    "            \")\"\n",
    "        ), f\"Invalid string representation of a position: {s}\"\n",
    "        line, column = s[1:-1].split(\",\")\n",
    "        line_nb = int(line)\n",
    "        column_nb = int(column)\n",
    "        return cls(line_nb, column_nb)\n",
    "\n",
    "    def __iter__(self) -> Generator[int, None, None]:\n",
    "        yield self.line_nb\n",
    "        yield self.column_nb\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr(tuple(self))\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.line_nb < other.line_nb or (\n",
    "                self.line_nb == other.line_nb and self.column_nb < other.column_nb\n",
    "        )\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self < other or self == other\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Premise:\n",
    "    \"\"\"Premises are \"documents\" in our retrieval setup.\"\"\"\n",
    "\n",
    "    path: str\n",
    "    \"\"\"The ``*.lean`` file this premise comes from.\n",
    "    \"\"\"\n",
    "\n",
    "    full_name: str\n",
    "    \"\"\"Fully qualified name.\n",
    "    \"\"\"\n",
    "\n",
    "    start: Pos = field(repr=False, compare=False)\n",
    "    \"\"\"Start position of the premise's definition in the ``*.lean`` file.\n",
    "    \"\"\"\n",
    "\n",
    "    end: Pos = field(repr=False, compare=False)\n",
    "    \"\"\"End position of the premise's definition in the ``*.lean`` file.\n",
    "    \"\"\"\n",
    "\n",
    "    # code: str = field(compare=False)\n",
    "    def __post_init__(self) -> None:\n",
    "        assert isinstance(self.path, str)\n",
    "        assert isinstance(self.full_name, str)\n",
    "        assert (\n",
    "                isinstance(self.start, Pos)\n",
    "                and isinstance(self.end, Pos)\n",
    "        )\n",
    "        assert (\n",
    "                self.start <= self.end\n",
    "        )\n",
    "        # assert isinstance(self.code, str) and self.code != \"\"\n",
    "\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Premise_corpus:\n",
    "    \"\"\"Premises are \"documents\" in our retrieval setup.\"\"\"\n",
    "\n",
    "    path: str\n",
    "    \"\"\"The ``*.lean`` file this premise comes from.\n",
    "    \"\"\"\n",
    "\n",
    "    full_name: str\n",
    "    \"\"\"Fully qualified name.\n",
    "    \"\"\"\n",
    "\n",
    "    start: Pos = field(repr=False, compare=False)\n",
    "    \"\"\"Start position of the premise's definition in the ``*.lean`` file.\n",
    "    \"\"\"\n",
    "\n",
    "    end: Pos = field(repr=False, compare=False)\n",
    "    \"\"\"End position of the premise's definition in the ``*.lean`` file.\n",
    "    \"\"\"\n",
    "    code: str = field(compare=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert isinstance(self.path, str)\n",
    "        assert isinstance(self.full_name, str)\n",
    "        assert (\n",
    "                isinstance(self.start, Pos)\n",
    "                and isinstance(self.end, Pos)\n",
    "        )\n",
    "        assert (\n",
    "                self.start <= self.end\n",
    "        )\n",
    "        assert isinstance(self.code, str) and self.code != \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the corpus structure\n",
    "\n",
    "The goal of this part is to iterate over the full corpus of .lean files. Per file, extract all used premises, store into a Premise object and then store in a mapping from Premise object to an id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_without_doc_string(code):\n",
    "    pattern = r\"/--(.*?)-/\\n\"\n",
    "    cleaned_code = re.sub(pattern, \"\", code, flags=re.DOTALL)\n",
    "    return cleaned_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DST_DIR = Path(\"/home/ex-anastasia/Premise-Retrieval/mathlib_handler_benchmark_410/\")\n",
    "corpus_path = DST_DIR / \"corpus.jsonl\"\n",
    "lines = list(corpus_path.open())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def_path: Mathlib/Analysis/SpecialFunctions/Trigonometric/ArctanDeriv.lean\n",
      "\n",
      "is_thm: True\n",
      "\n",
      "premise: \n",
      "\n",
      "premise_code: theorem continuousAt_tan {x : ‚Ñù} : ContinuousAt tan x ‚Üî cos x ‚â† 0 \n",
      "\n",
      "premise_goal: ContinuousAt Real.tan x ‚Üî Real.cos x ‚â† 0\n",
      "\n",
      "premise_context: ['x : ‚Ñù']\n",
      "\n",
      "moudel_id: {Premise(path='Mathlib/Analysis/SpecialFunctions/Trigonometric/ArctanDeriv.lean', full_name='Real.continuousAt_tan'): 4}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_line = json.loads(lines[2000])\n",
    "\n",
    "def_path = one_line['path']\n",
    "print(f'def_path: {def_path}\\n')\n",
    "\n",
    "# these are defined as globals over all lines\n",
    "corpus = [] # this will get written into config['dataset']['all_data_path'] + 'statement.jsonl',\n",
    "moudel_id = {} # this will get returned\n",
    "path_premise = {} # this is written into config['dataset']['all_data_path'] + 'module_id_mapping.json',\n",
    "count = 4 # this is simply the id of the premise as we loop over corpus\n",
    "\n",
    "# loop over premises in this one line\n",
    "premise_num = []\n",
    "premise = one_line['premises'][count]\n",
    "print(f\"is_thm: {premise['is_thm']}\\n\") # if this is false, we continue\n",
    "\n",
    "temp_dict = {}\n",
    "premise_state = {}\n",
    "premise_state['context'] = [item[1:-1] for item in premise['args']]\n",
    "premise_state['goal'] = premise['goal']\n",
    "premise['state'] = premise_state\n",
    "premise['code'] = get_code_without_doc_string(premise['code'])\n",
    "print(f\"premise: \\n\") # has code and state; state consists of goal and context\n",
    "print(f\"premise_code: {premise['code']}\\n\")\n",
    "print(f\"premise_goal: {premise_state['goal']}\\n\")\n",
    "print(f\"premise_context: {premise_state['context']}\\n\")\n",
    "temp_dict['id'] = count\n",
    "temp_dict['premise'] = premise\n",
    "temp_dict['def_path'] = def_path\n",
    "\n",
    "now_premise = Premise(def_path, premise['full_name'],\n",
    "                        Pos(premise['start'][0], premise['start'][1]),\n",
    "                        Pos(premise['end'][0], premise['end'][1]))\n",
    "p = now_premise\n",
    "full_name = p.full_name\n",
    "if full_name is None or \"user__.n\" in full_name or premise['code'] == \"\" or (full_name.startswith(\"[\") and full_name.endswith(\"]\")):\n",
    "    print('Error with full_name; skipping this one')\n",
    "\n",
    "if now_premise in moudel_id:\n",
    "    raise ValueError\n",
    "    # continue\n",
    "moudel_id[now_premise] = count # maps the now_premise object to a count\n",
    "print(f\"moudel_id: {moudel_id}\\n\")\n",
    "premise_num.append(count)\n",
    "corpus.append(temp_dict)\n",
    "path_premise[def_path] = premise_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the pre-processing\n",
    "\n",
    "The goal of this part is to iterate over the train dataset (or val, test) from Lean-Dojo. Each train element has a list of tactics with state_before, tactics, state_after. We extract these first and clean them up a bit into a dictionary with 'context' and 'goal', where the context is a list.  \n",
    "\n",
    "Then, we break the relation between (state, tactic, state), and just create a dictionary: \n",
    "\n",
    "`data_list[state]: {'context': [...], 'goal': '...'}`\n",
    "\n",
    "`data_list[premise]: [1,2,3]`\n",
    "\n",
    "Finally, the context and goal is what will get combined with `<VAR>` and `<GOAL>` tokens and this is what's processed by Bert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def_path: Mathlib/Topology/Compactness/Compact.lean\n",
      "now_tactic: {'state_before': 'X : Type u\\nY : Type v\\nŒπ : Type u_1\\ninst‚úù¬π : TopologicalSpace X\\ninst‚úù : TopologicalSpace Y\\ns t : Set X\\nl : Filter X\\nhs : IsCompact s\\n‚ä¢ Disjoint (ùìùÀ¢ s) l ‚Üî ‚àÄ x ‚àà s, Disjoint (ùìù x) l', 'state_after': 'X : Type u\\nY : Type v\\nŒπ : Type u_1\\ninst‚úù¬π : TopologicalSpace X\\ninst‚úù : TopologicalSpace Y\\ns t : Set X\\nl : Filter X\\nhs : IsCompact s\\nH : ‚àÄ x ‚àà s, Disjoint (ùìù x) l\\n‚ä¢ Disjoint (ùìùÀ¢ s) l', 'tactic': 'refine ‚ü®fun h x hx => h.mono_left <| nhds_le_nhdsSet hx, fun H => ?_‚ü©', 'premises': [{'full_name': 'Disjoint.mono_left', 'def_path': 'Mathlib/Order/Disjoint.lean', 'def_pos': [66, 8], 'def_end_pos': [66, 26]}, {'full_name': 'Iff.intro', 'def_path': '.lake/packages/lean4/src/lean/Init/Core.lean', 'def_pos': [116, 2], 'def_end_pos': [116, 7]}, {'full_name': 'nhds_le_nhdsSet', 'def_path': 'Mathlib/Topology/NhdsSet.lean', 'def_pos': [130, 8], 'def_end_pos': [130, 23]}]}\n",
      "\n",
      "annotated_tactic: [{'full_name': 'Disjoint.mono_left', 'def_path': 'Mathlib/Order/Disjoint.lean', 'def_pos': [66, 8], 'def_end_pos': [66, 26]}, {'full_name': 'Iff.intro', 'def_path': '.lake/packages/lean4/src/lean/Init/Core.lean', 'def_pos': [116, 2], 'def_end_pos': [116, 7]}, {'full_name': 'nhds_le_nhdsSet', 'def_path': 'Mathlib/Topology/NhdsSet.lean', 'def_pos': [130, 8], 'def_end_pos': [130, 23]}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open a line\n",
    "DST_DIR = Path(\"/home/ex-anastasia/Premise-Retrieval/mathlib_handler_benchmark_410/random/\")\n",
    "train_path = DST_DIR / \"train.jsonl\"\n",
    "train_lines = list(train_path.open())\n",
    "\n",
    "# this replicates _get_dataset_path \n",
    "data_list = [] \n",
    "not_in = set() \n",
    "\n",
    "# iterate over all train elements\n",
    "one_line_train = json.loads(train_lines[2000])\n",
    "\n",
    "traced_tactics = one_line_train['tactics'] # a list of tactics\n",
    "def_path = one_line_train['file_path']\n",
    "print(f\"def_path: {def_path}\")\n",
    "\n",
    "# each train element has a list of tactics, with state_before and state_after and tactic applied\n",
    "i = 0\n",
    "now_tactic = traced_tactics[i]\n",
    "annotated_tactic = now_tactic['premises']\n",
    "state_before = now_tactic['state_before']\n",
    "state_after = now_tactic['state_after']\n",
    "\n",
    "\n",
    "print(f\"now_tactic: {now_tactic}\\n\")\n",
    "print(f\"annotated_tactic: {annotated_tactic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_before: X : Type u\n",
      "Y : Type v\n",
      "Œπ : Type u_1\n",
      "inst‚úù¬π : TopologicalSpace X\n",
      "inst‚úù : TopologicalSpace Y\n",
      "s t : Set X\n",
      "l : Filter X\n",
      "hs : IsCompact s\n",
      "‚ä¢ Disjoint (ùìùÀ¢ s) l ‚Üî ‚àÄ x ‚àà s, Disjoint (ùìù x) l\n",
      "\n",
      "all_state_before: [{'context': ['X : Type u', 'Y : Type v', 'Œπ : Type u_1', 'inst‚úù¬π : TopologicalSpace X', 'inst‚úù : TopologicalSpace Y', 's t : Set X', 'l : Filter X', 'hs : IsCompact s'], 'goal': 'Disjoint (ùìùÀ¢ s) l ‚Üî ‚àÄ x ‚àà s, Disjoint (ùìù x) l'}]\n",
      "\n",
      "state_after: X : Type u\n",
      "Y : Type v\n",
      "Œπ : Type u_1\n",
      "inst‚úù¬π : TopologicalSpace X\n",
      "inst‚úù : TopologicalSpace Y\n",
      "s t : Set X\n",
      "l : Filter X\n",
      "hs : IsCompact s\n",
      "H : ‚àÄ x ‚àà s, Disjoint (ùìù x) l\n",
      "‚ä¢ Disjoint (ùìùÀ¢ s) l\n",
      "\n",
      "all_state_after: [{'context': ['X : Type u', 'Y : Type v', 'Œπ : Type u_1', 'inst‚úù¬π : TopologicalSpace X', 'inst‚úù : TopologicalSpace Y', 's t : Set X', 'l : Filter X', 'hs : IsCompact s', 'H : ‚àÄ x ‚àà s, Disjoint (ùìù x) l'], 'goal': 'Disjoint (ùìùÀ¢ s) l'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_state(state):\n",
    "    '''\n",
    "    Cleans up the state string, splits double new lines into list elements, and splits context and goal\n",
    "    '''\n",
    "    # removes full line that start with \"case\" -- WHY?\n",
    "    processed_state = re.sub(r\"^case.*(?:\\n|$)\", \"\", state, flags=re.MULTILINE)\n",
    "    split_states = processed_state.split(\"\\n\\n\")\n",
    "    proofstates = []\n",
    "    for s in split_states:\n",
    "        s = s.strip()\n",
    "        if s == \"no goals\": # if there are no goals, we append an empty context and \"no goals\"\n",
    "            proofstates.append({\"context\": [], \"goal\": \"no goals\"})\n",
    "            continue\n",
    "        if \"‚ä¢\" not in s: # if there is no goal, we skip\n",
    "            continue\n",
    "        try: # otherwise we split the context and goal based on \"‚ä¢\"\n",
    "            context_str, goal = s.split(\"‚ä¢\")\n",
    "            # remove new lines and extra spaces in the context and goal\n",
    "            context_str = re.sub(r\"\\n\\s+\", \" \", context_str).strip()\n",
    "            goal = re.sub(r\"\\n\\s+\", \" \", goal).strip()\n",
    "            # split the context into a list of strings based on \\n (every line is a context element)\n",
    "            context = list(filter(lambda v: \":\" in v, context_str.split(\"\\n\")))\n",
    "            proofstates.append({\"context\": context, \"goal\": goal})\n",
    "        except:\n",
    "            print(state, s)\n",
    "    return proofstates\n",
    "\n",
    "\n",
    "# let's look at what the state_before and state_after look like after process_state\n",
    "all_state_before = process_state(state_before)\n",
    "all_state_after = process_state(state_after)\n",
    "print(f\"state_before: {state_before}\\n\")\n",
    "print(f\"all_state_before: {all_state_before}\\n\")\n",
    "\n",
    "print(f\"state_after: {state_after}\\n\")\n",
    "print(f\"all_state_after: {all_state_after}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i don't run this one as i didn't compute full moudel_id\n",
    "premises_list = []\n",
    "for j in range(len(annotated_tactic)): # iterate over all used premises\n",
    "    now_premise = annotated_tactic[j]\n",
    "    # make into a premise object\n",
    "    premise = Premise(now_premise['def_path'], now_premise['full_name'],\n",
    "                        Pos(now_premise['def_pos'][0], now_premise['def_pos'][1]),\n",
    "                        Pos(now_premise['def_end_pos'][0], now_premise['def_end_pos'][1]))\n",
    "    # save if we hadn't extracted this guy from the corpus\n",
    "    if premise not in moudel_id:\n",
    "        not_in.add(premise)\n",
    "        continue\n",
    "    # otherwise append to list the id of the premise; so premises_list is a list of id's used in the proof\n",
    "    premises_list.append(moudel_id[premise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coming code, is the piece that 'destroys' the relation between state before, tactic, state after, as if just appends everything into the `data_list`. \n",
    "\n",
    "Specifically, the `data_list` looks like: \n",
    "\n",
    "`data_list[state]: {'context': [...], 'goal': '...'}`\n",
    "\n",
    "`data_list[premise]: [1,2,3]`\n",
    "\n",
    "where we thus have the context (variables), the goal of this premise and the list of premises applied. This facilitates the learning of: which premises can we apply given a particular context and a particular goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_list[state]: {'context': ['X : Type u', 'Y : Type v', 'Œπ : Type u_1', 'inst‚úù¬π : TopologicalSpace X', 'inst‚úù : TopologicalSpace Y', 's t : Set X', 'l : Filter X', 'hs : IsCompact s'], 'goal': 'Disjoint (ùìùÀ¢ s) l ‚Üî ‚àÄ x ‚àà s, Disjoint (ùìù x) l'}\n",
      "\n",
      "data_list[premise]: []\n",
      "\n",
      "data_list[module]: ['Mathlib/Topology/Compactness/Compact.lean']\n",
      "\n",
      "data_list[state_str]: X : Type u\n",
      "Y : Type v\n",
      "Œπ : Type u_1\n",
      "inst‚úù¬π : TopologicalSpace X\n",
      "inst‚úù : TopologicalSpace Y\n",
      "s t : Set X\n",
      "l : Filter X\n",
      "hs : IsCompact s\n",
      "‚ä¢ Disjoint (ùìùÀ¢ s) l ‚Üî ‚àÄ x ‚àà s, Disjoint (ùìù x) l\n",
      "\n",
      "data_list[state]: {'context': ['X : Type u', 'Y : Type v', 'Œπ : Type u_1', 'inst‚úù¬π : TopologicalSpace X', 'inst‚úù : TopologicalSpace Y', 's t : Set X', 'l : Filter X', 'hs : IsCompact s', 'H : ‚àÄ x ‚àà s, Disjoint (ùìù x) l'], 'goal': 'Disjoint (ùìùÀ¢ s) l'}\n",
      "\n",
      "data_list[premise]: []\n",
      "\n",
      "data_list[module]: ['Mathlib/Topology/Compactness/Compact.lean']\n",
      "\n",
      "data_list[state_str]: X : Type u\n",
      "Y : Type v\n",
      "Œπ : Type u_1\n",
      "inst‚úù¬π : TopologicalSpace X\n",
      "inst‚úù : TopologicalSpace Y\n",
      "s t : Set X\n",
      "l : Filter X\n",
      "hs : IsCompact s\n",
      "H : ‚àÄ x ‚àà s, Disjoint (ùìù x) l\n",
      "‚ä¢ Disjoint (ùìùÀ¢ s) l\n",
      "\n"
     ]
    }
   ],
   "source": [
    "premises_list = list(set(premises_list))\n",
    "premises_list = sorted(premises_list)\n",
    "\n",
    "# this iteration seems to only happen if we had inside one state_before multiple goals\n",
    "for k in range(len(all_state_before)):\n",
    "    now_state = all_state_before[k]\n",
    "    if now_state['goal'] != \"no goals\":\n",
    "        temp_dict = {}\n",
    "        temp_dict[\"state\"] = now_state\n",
    "        temp_dict['premise'] = premises_list\n",
    "        temp_dict['module'] = [def_path]\n",
    "        temp_dict['state_str'] = state_before\n",
    "        data_list.append(temp_dict)\n",
    "    print(f\"data_list[state]: {temp_dict['state']}\\n\")\n",
    "    print(f\"data_list[premise]: {temp_dict['premise']}\\n\")\n",
    "    print(f\"data_list[module]: {temp_dict['module']}\\n\")\n",
    "    print(f\"data_list[state_str]: {temp_dict['state_str']}\\n\")\n",
    "\n",
    "# same here but for state after\n",
    "for k in range(len(all_state_after)):\n",
    "    now_state = all_state_after[k]\n",
    "    if now_state['goal'] != \"no goals\":\n",
    "        temp_dict = {}\n",
    "        temp_dict[\"state\"] = now_state\n",
    "        temp_dict['premise'] = premises_list\n",
    "        temp_dict['module'] = [def_path]\n",
    "        temp_dict['state_str'] = state_after\n",
    "        data_list.append(temp_dict)\n",
    "    print(f\"data_list[state]: {temp_dict['state']}\\n\")\n",
    "    print(f\"data_list[premise]: {temp_dict['premise']}\\n\")\n",
    "    print(f\"data_list[module]: {temp_dict['module']}\\n\")\n",
    "    print(f\"data_list[state_str]: {temp_dict['state_str']}\\n\")\n",
    "\n",
    "# at the end of these steps, we have data_list which is a list of dictionaries, each dictionary has a state, premise, module, and state_str\n",
    "# what is the module? it is def_path, but why do we need it? it refers to the mathlib file from which we extracted this. I guess we want it for bookkeeping purposes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashable(data):\n",
    "    if isinstance(data, dict):\n",
    "        return frozenset((key, make_hashable(value)) for key, value in data.items())\n",
    "    elif isinstance(data, list):\n",
    "        return tuple(make_hashable(item) for item in data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "state_hashes = {}\n",
    "for temp_dict in data_list:\n",
    "    state = temp_dict[\"state\"]\n",
    "    premise = temp_dict[\"premise\"]\n",
    "    module = temp_dict[\"module\"]\n",
    "\n",
    "    state_hash = hash(make_hashable(state))\n",
    "    # hash(...) computes a unique integer hash for \"state\", allowing it to be used as a dictionary key.\n",
    "    \n",
    "    if state_hash in state_hashes:\n",
    "        matched_dict = state_hashes[state_hash]\n",
    "        matched_dict[\"premise\"].extend(premise)\n",
    "        matched_dict[\"module\"].extend(module)\n",
    "        matched_dict[\"premise\"] = list(set(matched_dict[\"premise\"]))\n",
    "        matched_dict[\"module\"] = list(set(matched_dict[\"module\"]))\n",
    "    else:\n",
    "        state_hashes[state_hash] = {\n",
    "            \"state\": state,\n",
    "            \"premise\": premise,\n",
    "            \"module\": module\n",
    "        }\n",
    "\n",
    "merged_data_list = list(state_hashes.values())\n",
    "# writes merged_data_list to random_our train file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't believe we use this expand logic. We only use: \n",
    "\n",
    "`train_file_path = \"./mathlib_handler_benchmark_410/random/random_our/pretrain_train_data.jsonl\"`\n",
    "\n",
    "`eval_file_path = \"./mathlib_handler_benchmark_410/random/random_our/pretrain_eval_data.jsonl\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {'context': ['X : Type u', 'Y : Type v', 'Œπ : Type u_1', 'inst‚úù¬π : TopologicalSpace X', 'inst‚úù : TopologicalSpace Y', 's t : Set X', 'l : Filter X', 'hs : IsCompact s'], 'goal': 'Disjoint (ùìùÀ¢ s) l ‚Üî ‚àÄ x ‚àà s, Disjoint (ùìù x) l'}, 'premise': [], 'module': ['Mathlib/Topology/Compactness/Compact.lean']}\n"
     ]
    }
   ],
   "source": [
    "# replicates logic of expand_data; uses the train file we just created\n",
    "expanded_data = []\n",
    "\n",
    "record = merged_data_list[0]\n",
    "\n",
    "# remember that record['premise'] is a list of premise id's\n",
    "# this logic iterates over the premise id's and creates a new record for each premise id of the form {\"premise\": premise_id, \"all_premises\": record['premise']}\n",
    "for premise_id in record[\"premise\"]:\n",
    "    new_record = record.copy()\n",
    "    new_record[\"premise\"] = premise_id\n",
    "    new_record['all_premises'] = record['premise']\n",
    "    expanded_data.append(new_record)\n",
    "# writes into train_expand_premise.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i skip this logic for now as its more or less the same as the _get_dataset_path part \n",
    "def _get_dataset_path_test(path, save_path, is_train, modeul_id):\n",
    "    data_list = []\n",
    "    not_in = set()\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            traced_tactics = data['tactics']\n",
    "            for i in range(len(traced_tactics)):\n",
    "                now_tactic = traced_tactics[i]\n",
    "                annotated_tactic = now_tactic['premises']\n",
    "                state_str = now_tactic['state_before']\n",
    "                state_before = process_state(now_tactic['state_before'])\n",
    "                premises_list = []\n",
    "                for j in range(len(annotated_tactic)):\n",
    "                    now_premise = annotated_tactic[j]\n",
    "                    premise = Premise(now_premise['def_path'], now_premise['full_name'],\n",
    "                                      Pos(now_premise['def_pos'][0], now_premise['def_pos'][1]),\n",
    "                                      Pos(now_premise['def_end_pos'][0], now_premise['def_end_pos'][1]))\n",
    "                    if premise not in modeul_id:\n",
    "                        not_in.add(premise)\n",
    "                        continue\n",
    "                    premises_list.append(modeul_id[premise])\n",
    "                if len(premises_list) == 0:\n",
    "                    continue\n",
    "                premises_list = list(set(premises_list))\n",
    "                premises_list = sorted(premises_list)\n",
    "                if state_before[0]['goal'] != \"no goals\":\n",
    "\n",
    "                    temp_dict = {}\n",
    "                    temp_dict['state'] = state_before\n",
    "                    temp_dict['premise'] = premises_list\n",
    "                    temp_dict['state_str'] = state_str\n",
    "                    data_list.append(temp_dict)\n",
    "                else:\n",
    "                    if len(state_before) != 0:\n",
    "                        raise ValueError\n",
    "    if is_train:\n",
    "        state_hashes = {}\n",
    "\n",
    "        for temp_dict in data_list:\n",
    "            state = temp_dict[\"state\"]\n",
    "            premise = temp_dict[\"premise\"]\n",
    "            state_str = temp_dict['state_str']\n",
    "            state_hash = hash(make_hashable(state))\n",
    "\n",
    "            # Flag to indicate if we've found a matching state in merged_data_list\n",
    "            if state_hash in state_hashes:\n",
    "                matched_dict = state_hashes[state_hash]\n",
    "                matched_dict[\"premise\"].extend(premise)\n",
    "            else:\n",
    "                state_hashes[state_hash] = {\n",
    "                    \"state\": state,\n",
    "                    \"premise\": premise,\n",
    "                    'state_str': state_str\n",
    "                }\n",
    "\n",
    "        merged_data_list = list(state_hashes.values())\n",
    "    else:\n",
    "        merged_data_list = data_list\n",
    "\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        for item in merged_data_list:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    print(len(merged_data_list))\n",
    "    print(f\"Data written to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final strings \n",
    "\n",
    "So we ended with a data_list with: \n",
    "\n",
    "`data_list[state]: {'context': [...], 'goal': '...'}`\n",
    "\n",
    "`data_list[premise]: [1,2,3]`\n",
    "\n",
    "For the first pre-train phase, we will simply translate `data_list[state]` into: \n",
    "\n",
    "`<VAR>c_1<VAR>c_2<VAR>...<GOAL>goal`\n",
    "and this is the string we will tokenise. \n",
    "\n",
    "Hence: in the pretrain phase, we do not use `data_list[premise]`. I presume we will use this in the RAG style finetuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import re\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_transform(object):\n",
    "    def __init__(self, shuffle_prob, remove_prob):\n",
    "        self.shuffle_prob = shuffle_prob\n",
    "        self.remove_prob = remove_prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        augmented_list = sample.copy()\n",
    "        if random.random() < self.shuffle_prob:\n",
    "            random.shuffle(augmented_list)\n",
    "        if len(sample) >= 15 and random.random() < self.remove_prob:\n",
    "            augmented_list = [elem for elem in augmented_list if random.random() > 0.2]\n",
    "\n",
    "        return augmented_list\n",
    "    \n",
    "\n",
    "def process_strings(string_list):\n",
    "    processed_list = []\n",
    "    for s in string_list:\n",
    "        s = \"<VAR>\" + s\n",
    "        s = re.sub(r'\\n\\s+', ' ', s)\n",
    "        processed_list.append(s)\n",
    "    result = ''.join(processed_list)\n",
    "    return result\n",
    "\n",
    "transform = data_transform(shuffle_prob=0, remove_prob=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ex-anastasia/Premise-Retrieval/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1945: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_dir = '/home/ex-anastasia/Premise-Retrieval/mathlib_handler_benchmark_410/random/random_our/tokenizer/'\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(tokenizer_dir, 'vocab.txt'), do_lower_case=False)\n",
    "special_tokens = [\"<VAR>\", \"<GOAL>\"]\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context raw: ['C : Type u', 'inst‚úù‚Å¥ : Category.{v, u} C', 'inst‚úù¬≥ : ConcreteCategory C', 'inst‚úù¬≤ : HasLimits C', 'inst‚úù¬π : ConcreteCategory.forget.ReflectsIsomorphisms', 'inst‚úù : PreservesLimits ConcreteCategory.forget', 'X : TopCat', 'F : Sheaf C X', 'Œπ : Type v', 'U : Œπ ‚Üí Opens ‚ÜëX', 's t : (CategoryTheory.forget C).obj (F.val.obj (op (iSup U)))', 'h : ‚àÄ (i : Œπ), (F.val.map (leSupr U i).op) s = (F.val.map (leSupr U i).op) t', 'sf : (i : Œπ) ‚Üí (CategoryTheory.forget C).obj (F.val.obj (op (U i))) := fun i => (F.val.map (leSupr U i).op) s', 'sf_compatible : IsCompatible F.val U sf']\n",
      "\n",
      "Goal raw: s = t\n",
      "\n",
      "Context cleaned: <VAR>C : Type u<VAR>inst‚úù‚Å¥ : Category.{v, u} C<VAR>inst‚úù¬≥ : ConcreteCategory C<VAR>inst‚úù¬≤ : HasLimits C<VAR>inst‚úù¬π : ConcreteCategory.forget.ReflectsIsomorphisms<VAR>inst‚úù : PreservesLimits ConcreteCategory.forget<VAR>X : TopCat<VAR>F : Sheaf C X<VAR>Œπ : Type v<VAR>U : Œπ ‚Üí Opens ‚ÜëX<VAR>s t : (CategoryTheory.forget C).obj (F.val.obj (op (iSup U)))<VAR>h : ‚àÄ (i : Œπ), (F.val.map (leSupr U i).op) s = (F.val.map (leSupr U i).op) t<VAR>sf : (i : Œπ) ‚Üí (CategoryTheory.forget C).obj (F.val.obj (op (U i))) := fun i => (F.val.map (leSupr U i).op) s<VAR>sf_compatible : IsCompatible F.val U sf\n",
      "\n",
      "Goal cleaned: s = t\n",
      "\n",
      "Combine: <VAR>C : Type u<VAR>inst‚úù‚Å¥ : Category.{v, u} C<VAR>inst‚úù¬≥ : ConcreteCategory C<VAR>inst‚úù¬≤ : HasLimits C<VAR>inst‚úù¬π : ConcreteCategory.forget.ReflectsIsomorphisms<VAR>inst‚úù : PreservesLimits ConcreteCategory.forget<VAR>X : TopCat<VAR>F : Sheaf C X<VAR>Œπ : Type v<VAR>U : Œπ ‚Üí Opens ‚ÜëX<VAR>s t : (CategoryTheory.forget C).obj (F.val.obj (op (iSup U)))<VAR>h : ‚àÄ (i : Œπ), (F.val.map (leSupr U i).op) s = (F.val.map (leSupr U i).op) t<VAR>sf : (i : Œπ) ‚Üí (CategoryTheory.forget C).obj (F.val.obj (op (U i))) := fun i => (F.val.map (leSupr U i).op) s<VAR>sf_compatible : IsCompatible F.val U sf<GOAL>s = t\n",
      "\n",
      "Tokenised: {'input_ids': [0, 4, 40, 31, 614, 89, 4, 743, 31, 652, 19, 95, 90, 17, 89, 97, 40, 4, 720, 31, 3119, 40, 4, 695, 31, 5571, 40, 4, 660, 31, 3119, 19, 1456, 19, 6180, 4, 619, 31, 5257, 3119, 19, 1456, 4, 61, 31, 1956, 4, 43, 31, 2422, 40, 61, 4, 129, 31, 614, 90, 4, 58, 31, 129, 229, 1321, 2291, 4, 87, 88, 31, 13, 677, 19, 1456, 40, 14, 19, 783, 13, 43, 19, 978, 19, 783, 13, 914, 13, 3432, 58, 14, 14, 14, 4, 76, 31, 241, 13, 77, 31, 129, 14, 17, 13, 43, 19, 978, 19, 712, 13, 13210, 58, 77, 14, 19, 914, 14, 87, 34, 13, 43, 19, 978, 19, 712, 13, 13210, 58, 77, 14, 19, 914, 14, 88, 4, 4641, 31, 13, 77, 31, 129, 14, 229, 13, 677, 19, 1456, 40, 14, 19, 783, 13, 43, 19, 978, 19, 783, 13, 914, 13, 58, 77, 14, 14, 14, 31, 34, 678, 77, 34, 35, 13, 43, 19, 978, 19, 712, 13, 13210, 58, 77, 14, 19, 914, 14, 87, 4, 4641, 68, 22480, 31, 7830, 43, 19, 978, 58, 4641, 5, 87, 34, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path_train = \"/home/ex-anastasia/Premise-Retrieval/mathlib_handler_benchmark_410/random/random_our/pretrain_train_data.jsonl\"\n",
    "data = datasets.load_dataset('json', data_files=file_path_train, split='train')\n",
    "\n",
    "item = 0 \n",
    "\n",
    "# here we iterate over the state context and goal\n",
    "context = data[item]['state']['context']\n",
    "goal = data[item]['state']['goal']\n",
    "\n",
    "is_train = True\n",
    "print(f'Context raw: {context}\\n')\n",
    "print(f'Goal raw: {goal}\\n')\n",
    "if is_train:\n",
    "    context = transform(context)\n",
    "context = process_strings(context)\n",
    "print(f'Context cleaned: {context}\\n')\n",
    "goal = re.sub(r'\\n\\s+', ' ', goal)\n",
    "print(f'Goal cleaned: {goal}\\n')\n",
    "combine = context + '<GOAL>' + goal\n",
    "print(f'Combine: {combine}\\n')\n",
    "input = tokenizer(combine, truncation=True, padding='max_length', max_length=512, return_special_tokens_mask=True)\n",
    "print(f'Tokenised: {input}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detokenized Text: [CLS] <VAR> C : Type u <VAR> inst‚úù‚Å¥ : Category. { v, u } C <VAR> inst‚úù¬≥ : ConcreteCategory C <VAR> inst‚úù¬≤ : HasLimits C <VAR> inst‚úù¬π : ConcreteCategory. forget. ReflectsIsomorphisms <VAR> inst‚úù : PreservesLimits ConcreteCategory. forget <VAR> X : TopCat <VAR> F : Sheaf C X <VAR> Œπ : Type v <VAR> U : Œπ ‚Üí Opens ‚ÜëX <VAR> s t : ( CategoryTheory. forget C ). obj ( F. val. obj ( op ( iSup U ) ) ) <VAR> h : ‚àÄ ( i : Œπ ), ( F. val. map ( leSupr U i ). op ) s = ( F. val. map ( leSupr U i ). op ) t <VAR> sf : ( i : Œπ ) ‚Üí ( CategoryTheory. forget C ). obj ( F. val. obj ( op ( U i ) ) ) : = fun i = > ( F. val. map ( leSupr U i ). op ) s <VAR> sf _ compatible : IsCompatible F. val U sf <GOAL> s = t [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Detokenize (convert back to text)\n",
    "detokenized_text = tokenizer.decode(input.input_ids)\n",
    "print(\"Detokenized Text:\", detokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the masking\n",
    "\n",
    "When we do the input into the model, we use the standard BertMaskedLM masking: \n",
    "\n",
    "`data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)`\n",
    "\n",
    "which masks as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input IDs: [0, 4, 40, 31, 614, 89, 4, 743, 31, 652, 19, 95, 90, 17, 89, 97, 40, 4, 720, 31, 3119, 40, 4, 695, 31, 5571, 40, 4, 660, 31, 3119, 19, 1456, 19, 6180, 4, 619, 31, 5257, 3119, 19, 1456, 4, 61, 31, 1956, 4, 43, 31, 2422, 40, 61, 4, 129, 31, 614, 90, 4, 58, 31, 129, 229, 1321, 2291, 4, 87, 88, 31, 13, 677, 19, 1456, 40, 14, 19, 783, 13, 43, 19, 978, 19, 783, 13, 914, 13, 3432, 58, 14, 14, 14, 4, 76, 31, 241, 13, 77, 31, 129, 14, 17, 13, 43, 19, 978, 19, 712, 13, 13210, 58, 77, 14, 19, 914, 14, 87, 34, 13, 43, 19, 978, 19, 712, 13, 13210, 58, 77, 14, 19, 914, 14, 88, 4, 4641, 31, 13, 77, 31, 129, 14, 229, 13, 677, 19, 1456, 40, 14, 19, 783, 13, 43, 19, 978, 19, 783, 13, 914, 13, 58, 77, 14, 14, 14, 31, 34, 678, 77, 34, 35, 13, 43, 19, 978, 19, 712, 13, 13210, 58, 77, 14, 19, 914, 14, 87, 4, 4641, 68, 22480, 31, 7830, 43, 19, 978, 58, 4641, 5, 87, 34, 88, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Masked Input IDs: tensor([[    0,     4, 30520,    31,   614,    89, 30520,   743,    31,   652,\n",
      "            19, 30520,    90,    17,    89,    97,    40,     4,   720,    31,\n",
      "          3119,    40,     4,   695,    31,  5571,    40,     4,   660,    31,\n",
      "          3119, 30520,  1456,    19,  6180,     4,   619,    31,  5257,  3119,\n",
      "            19,  1456, 30520,    61, 30520,  1956, 30520, 30520,    31, 30520,\n",
      "            40,    61,     4,   129,    31,   614,    90,     4,    58,    31,\n",
      "           129,   229,  1321,  2291,     4, 30520, 30520,    31,    13, 12438,\n",
      "            19,  1456,    40, 30520,    19,   783,    13,    43, 30520,   978,\n",
      "            19,   783,    13,   914,    13,  3432, 30520,    14,    14, 30520,\n",
      "             4,    76,    31,   241,    13,    77,    31, 30520,    14,    17,\n",
      "            13,    43, 30520,   978,    19,   712, 30520, 30520,    58, 30520,\n",
      "            14, 30520,   914,    14,    87, 30520, 30520, 30520, 30520,   978,\n",
      "            19,   712, 30520, 13210,    58,    77,    14, 30520,   914, 30520,\n",
      "         30520,     4,  4641, 30520,    13,    77, 30520,   129, 30520, 30520,\n",
      "            13, 30520,    19, 30520,    40,    14,    19,   783, 30520, 30520,\n",
      "            19,   978,    19,   783,    13, 30520,    13,    58,    77,  4062,\n",
      "            14,    14,    31,    34,   678,    77,    34,    35,    13,    43,\n",
      "            19,   978,    19,   712, 30520, 13210, 30520, 30520,    14,    19,\n",
      "           914,    14,    87,     4,  3362,    68, 22480,    31,  7830,    43,\n",
      "            19,   978, 30520,  4641,     5, 30520,    34,    88,     1,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2]])\n",
      "Labels (original tokens for masked positions): tensor([[ -100,  -100,    40,  -100,  -100,    89,     4,  -100,  -100,  -100,\n",
      "          -100,    95,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,    19,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,     4,  -100,    31,  -100,     4,    43,  -100,  2422,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,    87,    88,  -100,  -100,   677,\n",
      "          -100,  -100,  -100,    14,  -100,  -100,  -100,  -100,    19,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,    58,  -100,  -100,    14,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   129,  -100,  -100,\n",
      "          -100,  -100,    19,  -100,  -100,  -100,    13, 13210,  -100,    77,\n",
      "          -100,    19,  -100,  -100,  -100,    34,    13,    43,    19,  -100,\n",
      "          -100,  -100,    13,  -100,  -100,  -100,  -100,    19,  -100,    14,\n",
      "            88,  -100,  -100,    31,  -100,    77,    31,  -100,    14,   229,\n",
      "          -100,   677,  -100,  1456,  -100,  -100,  -100,  -100,    13,    43,\n",
      "          -100,  -100,  -100,  -100,  -100,   914,  -100,  -100,  -100,    14,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,    13,  -100,    58,    77,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  4641,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,    58,  -100,  -100,    87,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# Apply the collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "batch = [input]\n",
    "masked_batch = data_collator(batch)\n",
    "\n",
    "# note here: 30520 is the id of the <MASK>\n",
    "print(\"Original Input IDs:\", batch[0][\"input_ids\"])\n",
    "print(\"Masked Input IDs:\", masked_batch[\"input_ids\"])\n",
    "print(\"Labels (original tokens for masked positions):\", masked_batch[\"labels\"])\n",
    "# -100 gets ignored by the loss function; so we only care about the loss for the masked positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
