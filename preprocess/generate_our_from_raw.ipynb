{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Generator\n",
    "import re\n",
    "import toml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass(eq=True, unsafe_hash=True)\n",
    "class Pos:\n",
    "    \"\"\"Position in source files.\n",
    "\n",
    "    We use 1-index to keep it consistent with code editors such as Visual Studio Code.\n",
    "    \"\"\"\n",
    "\n",
    "    line_nb: int\n",
    "    \"\"\"Line number\n",
    "    \"\"\"\n",
    "\n",
    "    column_nb: int\n",
    "    \"\"\"Column number\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls, s: str) -> \"Pos\":\n",
    "        \"\"\"Construct a :class:`Pos` object from its string representation, e.g., :code:`\"(323, 1109)\"`.\"\"\"\n",
    "        assert s.startswith(\"(\") and s.endswith(\n",
    "            \")\"\n",
    "        ), f\"Invalid string representation of a position: {s}\"\n",
    "        line, column = s[1:-1].split(\",\")\n",
    "        line_nb = int(line)\n",
    "        column_nb = int(column)\n",
    "        return cls(line_nb, column_nb)\n",
    "\n",
    "    def __iter__(self) -> Generator[int, None, None]:\n",
    "        yield self.line_nb\n",
    "        yield self.column_nb\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr(tuple(self))\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.line_nb < other.line_nb or (\n",
    "                self.line_nb == other.line_nb and self.column_nb < other.column_nb\n",
    "        )\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self < other or self == other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Premise_corpus:\n",
    "    \"\"\"Premises are \"documents\" in our retrieval setup.\"\"\"\n",
    "\n",
    "    path: str\n",
    "    \"\"\"The ``*.lean`` file this premise comes from.\n",
    "    \"\"\"\n",
    "\n",
    "    full_name: str\n",
    "    \"\"\"Fully qualified name.\n",
    "    \"\"\"\n",
    "\n",
    "    start: Pos = field(repr=False, compare=False)\n",
    "    \"\"\"Start position of the premise's definition in the ``*.lean`` file.\n",
    "    \"\"\"\n",
    "\n",
    "    end: Pos = field(repr=False, compare=False)\n",
    "    \"\"\"End position of the premise's definition in the ``*.lean`` file.\n",
    "    \"\"\"\n",
    "    code: str = field(compare=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert isinstance(self.path, str)\n",
    "        assert isinstance(self.full_name, str)\n",
    "        assert (\n",
    "                isinstance(self.start, Pos)\n",
    "                and isinstance(self.end, Pos)\n",
    "        )\n",
    "        assert (\n",
    "                self.start <= self.end\n",
    "        )\n",
    "        assert isinstance(self.code, str) and self.code != \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Premise:\n",
    "    \"\"\"Premises are \"documents\" in our retrieval setup.\"\"\"\n",
    "\n",
    "    path: str\n",
    "    \"\"\"The ``*.lean`` file this premise comes from.\n",
    "    \"\"\"\n",
    "\n",
    "    full_name: str\n",
    "    \"\"\"Fully qualified name.\n",
    "    \"\"\"\n",
    "\n",
    "    start: Pos = field(repr=False, compare=False)\n",
    "    \"\"\"Start position of the premise's definition in the ``*.lean`` file.\n",
    "    \"\"\"\n",
    "\n",
    "    end: Pos = field(repr=False, compare=False)\n",
    "    \"\"\"End position of the premise's definition in the ``*.lean`` file.\n",
    "    \"\"\"\n",
    "\n",
    "    # code: str = field(compare=False)\n",
    "    def __post_init__(self) -> None:\n",
    "        assert isinstance(self.path, str)\n",
    "        assert isinstance(self.full_name, str)\n",
    "        assert (\n",
    "                isinstance(self.start, Pos)\n",
    "                and isinstance(self.end, Pos)\n",
    "        )\n",
    "        assert (\n",
    "                self.start <= self.end\n",
    "        )\n",
    "        # assert isinstance(self.code, str) and self.code != \"\"\n",
    "\n",
    "\n",
    "def get_code_without_doc_string(code):\n",
    "    pattern = r\"/--(.*?)-/\\n\"\n",
    "    cleaned_code = re.sub(pattern, \"\", code, flags=re.DOTALL)\n",
    "    return cleaned_code\n",
    "\n",
    "\n",
    "def _load_corpus_update(corpus_path, output_path_statement, output_path_module_id):\n",
    "    corpus = []\n",
    "    count = 0\n",
    "    moudel_id = {}\n",
    "    path_premise = {}\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            def_path = data['path']\n",
    "            premise_num = []\n",
    "            for premise in data['premises']:\n",
    "                if premise['is_thm'] == False:\n",
    "                    continue\n",
    "                temp_dict = {}\n",
    "                premise_state = {}\n",
    "                premise_state['context'] = [item[1:-1] for item in premise['args']]\n",
    "                premise_state['goal'] = premise['goal']\n",
    "                premise['state'] = premise_state\n",
    "                premise['code'] = get_code_without_doc_string(premise['code'])\n",
    "                temp_dict['id'] = count\n",
    "                temp_dict['premise'] = premise\n",
    "                temp_dict['def_path'] = def_path\n",
    "                now_premise = Premise(def_path, premise['full_name'],\n",
    "                                      Pos(premise['start'][0], premise['start'][1]),\n",
    "                                      Pos(premise['end'][0], premise['end'][1]))\n",
    "                p = now_premise\n",
    "                full_name = p.full_name\n",
    "                if full_name is None:\n",
    "                    continue\n",
    "                if \"user__.n\" in full_name or premise['code'] == \"\":\n",
    "                    # Ignore ill-formed premises (often due to errors in ASTs).\n",
    "                    continue\n",
    "                if full_name.startswith(\"[\") and full_name.endswith(\"]\"):\n",
    "                    # Ignore mutual definitions.\n",
    "                    continue\n",
    "                if count == 0:\n",
    "                    print(premise_state)\n",
    "                    print(now_premise)\n",
    "                if now_premise in moudel_id:\n",
    "                    raise ValueError\n",
    "                    # continue\n",
    "                moudel_id[now_premise] = count\n",
    "                premise_num.append(count)\n",
    "                count += 1\n",
    "                corpus.append(temp_dict)\n",
    "\n",
    "            if def_path in path_premise:\n",
    "                raise ValueError\n",
    "            path_premise[def_path] = premise_num\n",
    "\n",
    "    with open(output_path_statement, 'w', encoding='utf-8') as out_f:\n",
    "        for item in corpus:\n",
    "            out_f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    with open(output_path_module_id, 'w', encoding='utf-8') as f:\n",
    "        json.dump(path_premise, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return moudel_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_hashable(data):\n",
    "    if isinstance(data, dict):\n",
    "        return frozenset((key, make_hashable(value)) for key, value in data.items())\n",
    "    elif isinstance(data, list):\n",
    "        return tuple(make_hashable(item) for item in data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "def process_state(state):\n",
    "    processed_state = re.sub(r\"^case.*(?:\\n|$)\", \"\", state, flags=re.MULTILINE)\n",
    "    split_states = processed_state.split(\"\\n\\n\")\n",
    "    proofstates = []\n",
    "    for s in split_states:\n",
    "        s = s.strip()\n",
    "        if s == \"no goals\":\n",
    "            proofstates.append({\"context\": [], \"goal\": \"no goals\"})\n",
    "            continue\n",
    "        if \"âŠ¢\" not in s:\n",
    "            continue\n",
    "        try:\n",
    "            context_str, goal = s.split(\"âŠ¢\")\n",
    "            context_str = re.sub(r\"\\n\\s+\", \" \", context_str).strip()\n",
    "            goal = re.sub(r\"\\n\\s+\", \" \", goal).strip()\n",
    "            context = list(filter(lambda v: \":\" in v, context_str.split(\"\\n\")))\n",
    "            proofstates.append({\"context\": context, \"goal\": goal})\n",
    "        except:\n",
    "            print(state, s)\n",
    "    return proofstates\n",
    "\n",
    "\n",
    "def _get_dataset_path(path, save_path, modeul_id):\n",
    "    data_list = []\n",
    "    not_in = set()\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            traced_tactics = data['tactics']\n",
    "            def_path = data['file_path']\n",
    "            for i in range(len(traced_tactics)):\n",
    "                now_tactic = traced_tactics[i]\n",
    "                annotated_tactic = now_tactic['premises']\n",
    "                state_before = now_tactic['state_before']\n",
    "                state_after = now_tactic['state_after']\n",
    "                all_state_before = process_state(state_before)\n",
    "                all_state_after = process_state(state_after)\n",
    "                premises_list = []\n",
    "                for j in range(len(annotated_tactic)):\n",
    "                    now_premise = annotated_tactic[j]\n",
    "                    premise = Premise(now_premise['def_path'], now_premise['full_name'],\n",
    "                                      Pos(now_premise['def_pos'][0], now_premise['def_pos'][1]),\n",
    "                                      Pos(now_premise['def_end_pos'][0], now_premise['def_end_pos'][1]))\n",
    "                    if premise not in modeul_id:\n",
    "                        not_in.add(premise)\n",
    "                        continue\n",
    "                    premises_list.append(modeul_id[premise])\n",
    "                if len(premises_list) == 0:\n",
    "                    continue\n",
    "                premises_list = list(set(premises_list))\n",
    "                premises_list = sorted(premises_list)\n",
    "                for k in range(len(all_state_before)):\n",
    "                    now_state = all_state_before[k]\n",
    "                    if now_state['goal'] != \"no goals\":\n",
    "                        temp_dict = {}\n",
    "                        temp_dict[\"state\"] = now_state\n",
    "                        temp_dict['premise'] = premises_list\n",
    "                        temp_dict['module'] = [def_path]\n",
    "                        temp_dict['state_str'] = state_before\n",
    "                        data_list.append(temp_dict)\n",
    "\n",
    "                for k in range(len(all_state_after)):\n",
    "                    now_state = all_state_after[k]\n",
    "                    if now_state['goal'] != \"no goals\":\n",
    "                        temp_dict = {}\n",
    "                        temp_dict[\"state\"] = now_state\n",
    "                        temp_dict['premise'] = premises_list\n",
    "                        temp_dict['module'] = [def_path]\n",
    "                        temp_dict['state_str'] = state_after\n",
    "                        data_list.append(temp_dict)\n",
    "\n",
    "    state_hashes = {}\n",
    "\n",
    "    for temp_dict in data_list:\n",
    "        state = temp_dict[\"state\"]\n",
    "        premise = temp_dict[\"premise\"]\n",
    "        module = temp_dict[\"module\"]\n",
    "\n",
    "        state_hash = hash(make_hashable(state))\n",
    "\n",
    "        if state_hash in state_hashes:\n",
    "            matched_dict = state_hashes[state_hash]\n",
    "            matched_dict[\"premise\"].extend(premise)\n",
    "            matched_dict[\"module\"].extend(module)\n",
    "            matched_dict[\"premise\"] = list(set(matched_dict[\"premise\"]))\n",
    "            matched_dict[\"module\"] = list(set(matched_dict[\"module\"]))\n",
    "        else:\n",
    "            state_hashes[state_hash] = {\n",
    "                \"state\": state,\n",
    "                \"premise\": premise,\n",
    "                \"module\": module\n",
    "            }\n",
    "\n",
    "    merged_data_list = list(state_hashes.values())\n",
    "\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        for item in merged_data_list:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    print(f\"Data written to {save_path}\")\n",
    "    print(len(not_in))\n",
    "\n",
    "\n",
    "def _get_dataset_path_test(path, save_path, is_train, modeul_id):\n",
    "    data_list = []\n",
    "    not_in = set()\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            traced_tactics = data['tactics']\n",
    "            for i in range(len(traced_tactics)):\n",
    "                now_tactic = traced_tactics[i]\n",
    "                annotated_tactic = now_tactic['premises']\n",
    "                state_str = now_tactic['state_before']\n",
    "                state_before = process_state(now_tactic['state_before'])\n",
    "                premises_list = []\n",
    "                for j in range(len(annotated_tactic)):\n",
    "                    now_premise = annotated_tactic[j]\n",
    "                    premise = Premise(now_premise['def_path'], now_premise['full_name'],\n",
    "                                      Pos(now_premise['def_pos'][0], now_premise['def_pos'][1]),\n",
    "                                      Pos(now_premise['def_end_pos'][0], now_premise['def_end_pos'][1]))\n",
    "                    if premise not in modeul_id:\n",
    "                        not_in.add(premise)\n",
    "                        continue\n",
    "                    premises_list.append(modeul_id[premise])\n",
    "                if len(premises_list) == 0:\n",
    "                    continue\n",
    "                premises_list = list(set(premises_list))\n",
    "                premises_list = sorted(premises_list)\n",
    "                if state_before[0]['goal'] != \"no goals\":\n",
    "\n",
    "                    temp_dict = {}\n",
    "                    temp_dict['state'] = state_before\n",
    "                    temp_dict['premise'] = premises_list\n",
    "                    temp_dict['state_str'] = state_str\n",
    "                    data_list.append(temp_dict)\n",
    "                else:\n",
    "                    if len(state_before) != 0:\n",
    "                        raise ValueError\n",
    "    if is_train:\n",
    "        state_hashes = {}\n",
    "\n",
    "        for temp_dict in data_list:\n",
    "            state = temp_dict[\"state\"]\n",
    "            premise = temp_dict[\"premise\"]\n",
    "            state_str = temp_dict['state_str']\n",
    "            state_hash = hash(make_hashable(state))\n",
    "\n",
    "            # Flag to indicate if we've found a matching state in merged_data_list\n",
    "            if state_hash in state_hashes:\n",
    "                matched_dict = state_hashes[state_hash]\n",
    "                matched_dict[\"premise\"].extend(premise)\n",
    "            else:\n",
    "                state_hashes[state_hash] = {\n",
    "                    \"state\": state,\n",
    "                    \"premise\": premise,\n",
    "                    'state_str': state_str\n",
    "                }\n",
    "\n",
    "        merged_data_list = list(state_hashes.values())\n",
    "    else:\n",
    "        merged_data_list = data_list\n",
    "\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        for item in merged_data_list:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    print(len(merged_data_list))\n",
    "    print(f\"Data written to {save_path}\")\n",
    "\n",
    "\n",
    "def expand_data(input_file, output_file):\n",
    "    expanded_data = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line.strip())\n",
    "            for premise_id in record[\"premise\"]:\n",
    "                new_record = record.copy()\n",
    "                new_record[\"premise\"] = premise_id\n",
    "                new_record['all_premises'] = record['premise']\n",
    "                expanded_data.append(new_record)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for record in expanded_data:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Data written to {output_file}\")\n",
    "    print(len(expanded_data))\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser_config = argparse.ArgumentParser()\n",
    "    parser_config.add_argument(\n",
    "        \"--config_path\", required=True, type=str, help=\"Path to the config file\"\n",
    "    )\n",
    "    args_config = parser_config.parse_args()\n",
    "    config = toml.load(args_config.config_path)\n",
    "    all_path_ori = config['dataset']['parent_path']\n",
    "    all_path_save = config['dataset']['parent_save_path_our']\n",
    "    if not os.path.exists(all_path_save):\n",
    "        os.makedirs(all_path_save)\n",
    "\n",
    "    train_path = all_path_ori + \"train.jsonl\"\n",
    "    test_path = all_path_ori + \"test.jsonl\"\n",
    "    val_path = all_path_ori + \"valid.jsonl\"\n",
    "    train_path_save = all_path_save + 'train.jsonl'\n",
    "    test_path_save = all_path_save + 'test.jsonl'\n",
    "    val_path_save = all_path_save + 'val.jsonl'\n",
    "    test_path_save_increase = all_path_save + 'test_increase.jsonl'\n",
    "    val_path_save_increase = all_path_save + 'val_increase.jsonl'\n",
    "    train_expand_path = all_path_save + \"train_expand_premise.jsonl\"\n",
    "\n",
    "    modeul_id = _load_corpus_update(\n",
    "        config['dataset']['all_data_path'] + \"corpus.jsonl\",\n",
    "        config['dataset']['all_data_path'] + 'statement.jsonl',\n",
    "        config['dataset']['all_data_path'] + 'module_id_mapping.json',\n",
    "    )\n",
    "\n",
    "    _get_dataset_path(train_path, train_path_save, modeul_id)\n",
    "    _get_dataset_path(test_path, test_path_save, modeul_id)\n",
    "    _get_dataset_path(val_path, val_path_save, modeul_id)\n",
    "\n",
    "    expand_data(train_path_save, train_expand_path)\n",
    "\n",
    "    _get_dataset_path_test(test_path, test_path_save_increase, is_train=False, modeul_id=modeul_id)\n",
    "    _get_dataset_path_test(val_path, val_path_save_increase, is_train=False, modeul_id=modeul_id)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
